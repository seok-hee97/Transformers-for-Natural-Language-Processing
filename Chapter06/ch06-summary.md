# CH06 OpenAI GPT-2 및 GPT-3 모델을 사용한 텍스트 생성

다룰 주제
- 원본 트랜스포머 아키텍처의 한계
- 리포머가 트랜스포머의 한계를 해결하는 방법
- PET가 트랜스포머 훈련의 한계를 해결하는 방버
- 제로-샷 트랜스포머 모델 정의
- 퓨-샷에서 원-샷으로 가는 경로
- GPT-2 및 GPT-3 모델
- 인간에 가까운 GPT-2 텍스트 완성 모델 구축
- 345M 파라미터 모델 구현 및 실행
- 표준 모델로 GPT-2와 상호 작용
- 언어 모델링 GPT-2 117M 파라미터 모델 교육
- 맞춤형 특정 데이터셋 가져오기
- 맞춤형 데이터셋 인코딩
- 모델 컨디셔닝(conditioning)
- 특정 텍스트 완성 작업을 위한 GPT-2 모델 컨디셔닝


### 6.1 10억 파라미터 트랜스포머 모델의 부상   


#### 6.1.1 트랜스포머 모델의 크기 증가


##### A. 컨텍스트 크기 및 최대 경로 길이   

트랜스포머 모델의 주출돌은 어텐션 서브레이어에 있음.     
그리고 어텐션 서브레이어의 핵심 속성은 컨텍스트 크기를 처리하는 데 사용하는 방법


- table 초대 경로 길이
| Layer Type |  Maximum Path Length  | Context Size |
| ---------- | --------------------- | ----------- |
| Self-Attention | O(1)              | 1            |
| Recurrent      | O(n)              |  12288       |


어텐션은 연산을 일대일 토큰 연산으로 끌어내렸음.   
모든 레이어가 같기 떄문에 트랜스포머 모델의 크기를 훨씬 쉽게 확장가능



#### 6.2 트랜스포머,리포머,PET, 또는 GPT?



##### 6.2.1 원본 트랜스포머 아키텍처의 한계

원본 트랜스포머 모델의 한계는 더 높은 기계 성능으로 이끄는 메모리 문제와 연결됨    

- A.BertViz 실행
  트랜스포머 어텐션 헤드들을 시가화하고, 그들과 상호작용하고, 트랜스포머 모델의   
  한계를 이해하는 데 4 단계만 거치면 된다.



***head_view_bery.ipynb***



##### 6.2.2 리포머

- Kitaev 등은 기존 트랜스포머 모델에 기능을 추가하여 어텐션 문제와   
  메모리 문제를 해결하도록 리포머 설계     


- 리포머는 먼저 LSH(Locality Sensitivity Hashing) 버킷과    
  청킹(buckets and chunking)으로 어텐션 문제를 해결


- LSH는 데이터셋에서 가장 가까운 이웃들을 검색. 해쉬 함수는 데이터 포인트 q가 p에    
  가까우면 hash(q) == hash(p)로 결정.   
  데이터 포인트는 트랜스포머 모델 해드의 키

- 리포머는 트랜스포머의 한계에 대한 특효약이 아님을 명심



##### 6.2.3 PET(Pattern-Exploiting Training. 패턴 활용 훈련)    

- PET를 사용한 양뱡향 ALBERT 모델이 GPT-3 단뱡향 모델보다 성능이 우수함을 주목


##### A.PET의 철학

- PET의 한 가지 핵심 원칙
  ```훈련 작업을 빈칸 메우기 질문으로 재표현(reformulate)하라```

- PET가 입력을 출력으로 매핑하는 프로세스에는 PVPs(pattern verbalizer pairs) 집합 필요    
  - 패턴 P : 입력을 단일 마스크가 포함된 빕ㄴ칸 메우기 질문으로 전환(매핑)
  - verbalizer v: 각 출력을 단일 토큰으로 전환(매핑)   




#### 6.4 OpenAI GPT 모델의 아키텍처    


##### 6.4.1 미세 조정에서 제로-샷 모델까지    


- 미세 조정(FT)은 앞 장들에서 살펴본 의미에서 수행    
  트랜스포머 모델은 훈련된 다음 다운스트림 작업에 대해 미세 조정됨   
  점진적으로 0으로 줄임

- Few-Shot(FS)은 큰 진전을 의미. 모델을 추론할때,    
  컨디셔닝(conditioning)으로서 수행할 작업의 데모가 제시.     
  컨디셔닝은 GPT 팀이 프로세스에서 제외한 가중치 업데이트를 대체    

- One-Shot(1S)은 프로세스를 한 단계 더 발전시킴.    
  훈련된 GPT 모델에는 수행할 다운스트림 작업의 데모가 하나만 제공   
  가중치 업데이트 역시 허용 x.     


- Zero-Shot(ZS)은 궁극적인 목표. 훈련된 GPT 모델은 수행할    
  다운스트림 작업에 대한 데모가 제공 x




##### 6.4.2 디코더 레이어 쌓기
언어 모델링에 초점을 맞췄음   
디코더 뿐인 트랜스포머 모델의 크기를 크게 늘림    

텍스트 및 위치 임베딩 서브레이어, 마스킹된 멀티 헤드 셀프 어텐션 레이어,     
정규화 서브레이어, 피드포워드 서브레이어 및 출력을 볼 수 있음    



#### 6.5 GPT-2에 의한 텍스트 완성


***OpenAI_GPT_2.ipynb***

