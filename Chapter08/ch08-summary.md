# [CH08 Tokenizer & DATASET Matching]    


#### [Subject]   

- 토크나이저의 출력을 제어하기 위한 기본 지침   
- 원시 데이터 전략 및 전처리 데이터 전략   
- Word2vector 토큰화 문제 및 한계   
- word2vector 토크나이저를 평가하는 Python 프로그램 만들기   
- GPT-2 토크나이저 평가   
- 바이트 수준 BPE 알고리즘의 출력을 평가하는 Python 프로그램 빌드   
- 특정 어휘로  NLP 작업 사용자 정의(customizing)   
- 표준 T5 조건부 입력 샘플 테스트   
- 데이터셋 개선  


#### 8.1 토크나이저와 데이터셋 매칭

트랜스포머 훈련을 위해 벤치마크 데이터셋을 다운받는 이점     
모든 연구실에서 똑같은 레퍼런스를 사용   
모델의 성능을 같은 데이터를 사용해 다른 모델들과 비교 가능    



#### 8.2 최선 관행    

기업 품질 관리(QC) 최선 관행을 전처리 및 후처리 단계에 적용    
기술된 예제는 수용가능한 실제 프로젝트 데이터셋을 얻기 위해    
요구되는 엄청난 작업에 대한 아이디어 제공     


품질관리는 전처리 단계(step1. 트랜스포머 훈련때)와 후처리 단계(step2. 트랜스포머 생산 때)   

##### 단계 1: 전처리     
  
데이터셋을 사용하기 전에 몇가지 표준 휴리스틱(heuristics)을 적용    

- 구두점이 있는 문장
  마침표나 물음표와 같은 구두점으로 끝나느 문장을 선택하는 것이 좋음   
- 나쁜 단어 제거     
  나쁜 단어들을 제거 
  https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words     

- 코드 제거     
  일반저긍로 NLP 작업을 위해서 콘텐트에서 코드를 제거하는 것이 최선    

- 언어 감지    
  떄떄로 웹사이트에는 기본 "lorem ipsum(로렘 입숨)" 텍스트가 돌어있는 페이지가 있음.   
  데이터셋의 모든 콘텐트가 원하는 언어로 되어 있는지 확인해야 함      
  시작하는 가장 좋은 방법은 50개 이상의 언어를 감지할 수 있는 langdetect를 사용.    
  langdetect :https://pypi.org/project/langdetect/   

- 차별 언급 삭제 
  이것은 필수!! -> 윤리적인 기계 원함

- 로직 체크    
  의미가 없는 문장을 필터링하기 위해 자연어 추론(NLI)을 수행하는     
  훈련된 트랜스포ㅓ 모델을 데이터셋에 대해 실행하는 것이 좋음      

- 잘못된 정보 참조     
  작동하지 않는 링크, 비윤리적인 웹 사이트 또는 사람을 가리키는 텍스트를 제거    
  힘든일이잔 충분히 가치 있음    


추가 조치 :
개인 정보 보호법 위반 필터링, 특정 프로젝트를 위한 기타 조치 등도 필요     


##### 단계 2: 후처리    

트랜스포머는 역사상 가장 강력한 NLP 모델    
수용할 수 없는 작업을 실행하기 위해 모델이 수행하는 NLP작업의 무기화를 피해야 함   

- 실시간으로 입력 텍스트 확인    
  나쁜 정보를 받아들이지말라. 실시간으로 입력을 구문 분석(parse)하고,    
  수용 불가한 데이터를 필터링    

- 실시간 메시지
  사용자가 로그를 참고할 수 있도록, 거부된 데이터를 필터링된 이유와 함께 저장    

- 언어 전환   
  드문 어휘는 가능하면 표준 어휘로 바꿈    
  항상 가능한 작업은 아니지만 가능할 떄는 개선에 도움이 됨    

- 개인 정보 확인    
  실행되는 국아ㅔ서 승인하지 않는 한 개인 데이터는 데이터셋 및 작업에서   
  제외되어야 한다. 어려운 주제     


##### A.연속적 인간 품질 관리      
트랜스포머는 복잡한 NLP 작업의 대부분을 점진적으로 인수    
그러나 인간의 개입은 여전히 필수적    

올바른 접근 방식은 트랜스포머를 훈련시키고, 구현하고, 출력을 제어하고, 중요한 결과를     
훈련세트에 다시 공급하는 것    
훈련세트는 지속적으로 개선되며, 트랜스포머는 계속 학습    


step 1: Preprocessing Training.  
step 2: Post-processing Production.   
step 3: Continuous Human Quality Control.   


#### 8.1.2 Word2Vec 토큰화      


***Tokenizer.ipynb***



#### 8.2 특정 어휘를 사용한 표준 NLP 작업     

토큰화 섹션 사레 3: 드문 단어 및 사례, 사레 4:드문 단어 대체에 초첨      


ch06 gpt2,gpt3 모델을 사용한 텍스트 생성세서 데이터셋을 훈련하는데 사용한 노투북의    
변경 버전 

***Training_OpenAI_GPT_2_CH08.ipynb*** 를 사용     


##### 8.2.1 GPT-2에 의한 비조건부 샘플 생성     





#### 8.3 T5 권리장전 샘플     

***Summarizing_Text_with_T5.ipynb*** 노트북 사본인    
***Summarizing_Text_V2.ipynb***.      


##### 8.3.1 권리장전 요약 1



##### 8.3.2 권리장전 요약 2


