#  CH01 : Getting Started with the Model Architeture of the Transformer




- The background of the Transformer
- The architecture of the Transformer
- The Transformer's self-attention model
- The encoding and decoding stacks
- Input and output embedding
- Positional embedding
- Self-attention
- Multi-head attention
- Masked multi-attention
- Residual connections
- Normalization
- Feddforward network
- Output probabilites






- 개발/NLP(Natural Language Processing)
[논문 리뷰] Attention Is All You Need, Transformer
https://simonezz.tistory.com/65



트랜스포머 아키텍처 -> 놀라운 원거리 종속성(long-distance dependencies) 조사로 시작

트랜스포머 NLU(Natural Language Understanding) 새로운 방식으로  문서 및 구두 시퀀스에서 의미있는 표현으로 변환(transduction)을 수행